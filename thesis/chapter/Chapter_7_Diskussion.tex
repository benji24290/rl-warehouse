\chapter{Diskussion}

Die Resultate aus Experiment 1 zeigen, dass es für Reinforcement Learning in dieser minimalen Umgebung durchaus möglich ist, eine gute Policy zu erlernen und die Performanz der heuristisch definierten Policy zu übertreffen. Die durch Sarsa erlernte Policy hat dabei die Resultate der heuristischen Strategie um 30 Prozent übertroffen. Ausserdem hat der Q-Learning-Agent in diesem Experiment eine Policy erlernt, welche beim Erstellen des Environments nicht so eingeplant wurde. Der Agent hat festgestellt, dass Artikel in der Ankunft keine Lagerkosten generieren und dies dementsprechend mit mittlerem Erfolg genutzt. Dabei lässt sich die erste Forschungsfrage mit der Begrenzung auf das untersuchte Environment folgendermassen beantworten: Die Performanz, der durch Reinforcement Learning erlernten Strategie, übertrifft die der heuristischen Strategie.
Im zweiten Experiment wurde die Komplexität erhöht. Dabei konnte erneut die Performanz der Heuristik übertroffen werden. Die durch Q-Learning erlernte Policy erreichte einen durchschnittlichen Step-Reward der 67 Prozent höher war als der der Heuristik. Des Weiteren ist aber deutlich zu erkennen, dass durch das Wachstum des State-Action-Spaces eine höhere Anzahl an Episoden nötig war, um alle State-Action-Paare zu besuchen. Die zweite Forschungsfrage kann aufgrund dieser Ergebnisse wie folgt beantwortet werden: Die Performanz, der durch Reinforcement Learning erlernten Strategie, konnte die der heuristischen Strategie übertreffen. Dabei kann jedoch nicht auf ein reelles Lager generalisiert werden – dazu wurden zu wenige Artikel und Lagerplätze initialisiert. Eine Erweiterung um einen Artikel hat eine hohe Auswirkung auf den State. Die Information, welcher Artikel auf Lager ist, wird benötigt, um festzustellen, welche Artikel öfter bestellt werden. Eine Möglichkeit, die Komplexität drastisch zu verringern, wäre diese: Die States vom Environment werden auf 0 und 1 reduziert, wobei 1 bedeutet, es ist ein Artikel im Lager, in der Ankunft oder wurde soeben bestellt. Die Information über die Häufigkeit könnte mit einem Array, welches aus den letzten $n$ bestellten Artikel besteht, gespeichert werden.
Die in dieser Arbeit erzielten Ergebnisse dienen zur Grundlage und bieten als Artefakt ein erweiterbares Environment, welches mit geringem Aufwand weiter skaliert werden kann. Die nächste sinnvolle Untersuchung wäre eine Verkleinerung des State-Spaces. Ausserdem sollten weiterführende Algorithmen, welche sich in komplexeren Environments beweisen können – wie in den Atari-Spielen \cite{mnih2013playing} – verwendet werden. So wird häufig mit Deep Q-Learning ein Neuronales Netz erstellt anstatt einer Q-Matrix, welches besser auf unbesuchte State-Action-Paare generalisieren kann.
Ein Einsatz in einem reellen Lager könnte erst nach weiteren Untersuchungen abschliessend abgeschätzt werden. 
